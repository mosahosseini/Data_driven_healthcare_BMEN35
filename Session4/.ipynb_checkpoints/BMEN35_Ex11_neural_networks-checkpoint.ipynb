{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35_2023/blob/main/Session4/BMEN35_Ex11_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxN0VBlzuaWg"
   },
   "source": [
    "## Neural networks\n",
    "In this notebook we will have a look at neural network. The \"old school\" kind, shallow networks.\n",
    "\n",
    "As usual, we will start by import some libraries / modules, generating some data and visualising it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyEFI9Biuu06"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X,y = make_moons(n_samples = 300, noise=0.3, random_state=0)\n",
    "c = np.array(['r','b'])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=c[y])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAHaXHjUu-kE"
   },
   "source": [
    "So, we have two classes (\"red\" and \"blue\") and two features and from the scatter-plot and it  looks like they are not linearly separable.\n",
    "\n",
    "What we can do is to use a neural network. We will use 1 hidden layer.\n",
    "\n",
    "Lets start with some bokkkeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "217bi0GBwm25"
   },
   "outputs": [],
   "source": [
    "input_dim = X.shape[1] # Input dimensions, number of \"features\", 2 in this case\n",
    "num_classes = np.size(np.unique(y)) # Number of classes (2), binary in this case\n",
    "hidden_layer_size = 64 # no of neurons in hidden layer\n",
    "# initialize parameters\n",
    "W1 = 0.01 * np.random.randn(input_dim,hidden_layer_size) #  Weigths for input layer\n",
    "b1 = np.zeros((1,hidden_layer_size))             #  biases for input layer\n",
    "W2 = 0.01 * np.random.randn(hidden_layer_size,num_classes) #  Weigths for hidden layer\n",
    "b2 = np.zeros((1,num_classes))             #  biases for hidden layer\n",
    "\n",
    "eta = 0.1 # Learning rate\n",
    "iterations = 25000 # How many times we will run our loop, same as epochs\n",
    "num_examples = X.shape[0]\n",
    "loss = np.zeros(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSscn6v9yAnV"
   },
   "source": [
    "Now we have finished with our assigning of parameters and set other things. Let start with our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiOX6NqEyWPC"
   },
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "  L0 = X # Input layer\n",
    "  L1_in = np.dot(L0, W1) + b1 # Hidden layer input\n",
    "  L1_out = np.maximum(0, L1_in) # Hidden layer output, we use ReLU activation function here\n",
    "  L2_in = np.dot(L1_out, W2) + b2 # Output layer input\n",
    "  L2_out = np.exp(L2_in)/ np.sum(np.exp(L2_in), axis=1, keepdims=True) # Output layer output, there is also a softmax function in scipy.special\n",
    "\n",
    "  cost =  -np.log(L2_out[range(num_examples),y])\n",
    "  loss[i] = np.sum(cost)/num_examples\n",
    "\n",
    "  # compute the gradient on L2out (6.26b in the book)\n",
    "  dL2out = L2_out\n",
    "  dL2out[range(num_examples),y] -= 1\n",
    "  dL2out = dL2out / num_examples\n",
    "\n",
    "  # backpropate the gradient to the parameters\n",
    "  # first backprop into parameters W2 and b2\n",
    "  dW2 = np.dot(L1_out.T, dL2out)\n",
    "  db2 = np.sum(dL2out, axis=0, keepdims=True)\n",
    "  # next backprop into hidden layer\n",
    "  dL1 = np.dot(L2_out, W2.T)\n",
    "  # backprop through ReLU non-linearity\n",
    "  dL1[L1_out <= 0] = 0\n",
    "  # finally into W,b\n",
    "  dW1 = np.dot(X.T, dL1)\n",
    "  db1 = np.sum(dL1, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "  # update weights and biases\n",
    "  W1 = W1 -eta * dW1\n",
    "  b1 = b1 -eta * db1\n",
    "  W2 = W2 -eta * dW2\n",
    "  b2 = b2 -eta * db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVbw_8JBWEt3"
   },
   "source": [
    "Now that we have finished, lets check the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qF0Nsc05y-cB"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhwv3CAj4kJX"
   },
   "source": [
    "Alright, looks ok. The loss seems to have decreased. Lets check what we have for training accuracy. We will do a forward pass of the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMoTM8T14yJw"
   },
   "outputs": [],
   "source": [
    "L1_out = np.maximum(0, np.dot(X, W1) + b1)\n",
    "L2_out = np.dot(L1_out, W2) + b2\n",
    "y_hat = np.argmax(L2_out, axis=1)\n",
    "accuracy = np.sum(y_hat==y)/y.size # Calculate the classification accuracy\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlp1BG8oXR1R"
   },
   "source": [
    "We seem to go quite good accuracy even though the data was not linearly separable. That is it for the introduction to neural networks tutorial.\n",
    "\n",
    "##The end"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
