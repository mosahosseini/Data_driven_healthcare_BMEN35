{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133a523d",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35_2023/blob/main/Session3/BMEN35_Ex9_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31bd1aa",
   "metadata": {
    "id": "a31bd1aa"
   },
   "source": [
    "## Logistic regression\n",
    "\n",
    "In this notebook we will \"look under the hood\" on Logistic Regression. We will start with importing some of the typical libraries and generate some data.\n",
    "\n",
    "Remember that logistic regression is used for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7916ace",
   "metadata": {
    "id": "c7916ace"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Four features x1,x2,x3,x4 and two classes (binary classification)\n",
    "X, y = make_classification(n_samples = 1000,n_features=4, n_classes = 2)\n",
    "\n",
    "# Lets split the data into a train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a658ad",
   "metadata": {
    "id": "b2a658ad"
   },
   "source": [
    "As stated in the book there is no analytical solution to Logistic Regression so we will make use of another tool in our toolbox, that is Gradient Descent.\n",
    "\n",
    "Lets start with some bookeeping and add an intercept term for our training and test data and initialise the theta vector to zero. We have four features x1, x2, x3, x4. The equivalent Linear Regression model would be the following with x0 = 1 (our intercept).\n",
    "\n",
    "\\begin{equation}\n",
    "z = \\theta_0x_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3 + \\theta_4x_4 = \\bf{\\theta^Tx}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f344a",
   "metadata": {
    "id": "550f344a"
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train_i = np.c_[np.ones((np.shape(X_train)[0],1)),X_train] # Add 1 for intercept theta_0\n",
    "X_test_i = np.c_[np.ones((np.shape(X_test)[0],1)),X_test] #Add 1 for intercept theta_0\n",
    "theta = np.zeros((np.shape(X_train_i)[1])) # Intialize theta to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399e080",
   "metadata": {
    "id": "0399e080"
   },
   "source": [
    "The next thing to do is the actual algorithm for Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b95c7b",
   "metadata": {
    "id": "f2b95c7b"
   },
   "outputs": [],
   "source": [
    "# Lets again use gradient descent to find the right values for theta\n",
    "n_epochs = 30 # The number of epoch\n",
    "eta = 0.01 # Our learning rate\n",
    "J = np.zeros(n_epochs)\n",
    "for i in range(n_epochs):\n",
    "    theta_T_X = np.dot(X_train_i, theta)  # X*theta\n",
    "    #p_hat = 1 / (1 + np.exp(-theta_T_X))  # We pass this through the logistic function\n",
    "    p_hat = np.exp(theta_T_X) / (1 + np.exp(theta_T_X))\n",
    "    error = y_train - p_hat\n",
    "    J[i] = np.sum(-(y_train*np.log(p_hat) + (1-y_train)*np.log(1-p_hat)))/len(y_train)\n",
    "    grad = np.dot(X_train_i.T, error)\n",
    "    theta = theta + eta * grad\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b401954",
   "metadata": {
    "id": "6b401954"
   },
   "source": [
    "Now we have found our thetas. Lets use them to make prediction on the test set. We will do that in the following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954096b",
   "metadata": {
    "id": "7954096b"
   },
   "outputs": [],
   "source": [
    "y_hat = 1 / (1 + np.exp(-np.dot(X_test_i, theta))) # Calculate probabilities using our thetas.\n",
    "y_hat = np.round(y_hat) # Easy cheat instead of using if statements or similar\n",
    "acc = np.sum(y_hat == y_test)/len(y_test) # Calculate accuracy\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a775f999",
   "metadata": {
    "id": "a775f999"
   },
   "source": [
    "## The end"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
