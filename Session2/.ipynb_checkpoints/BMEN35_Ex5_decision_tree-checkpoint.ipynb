{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35_2023/blob/main/Session2/BMEN35_Ex5_decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHJGZDmrGzI4"
   },
   "source": [
    "## Decision Tree\n",
    "In this notebook we will be using decision trees. And we will use the example from the book (Example 2.6).\n",
    "\n",
    "Lets first import the libraries and generate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5-b-XJZLUHZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np # Them good ol' libraries we always use\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([[9,2],[1,4],[4,6],[4,1],[1,2],[1,8],[6,4],[7,9],[9,8],[9,6]]) # x1,x2 pair for the data\n",
    "y = np.array([0,0,0,0,0,1,1,1,1,1]) # the two different classes of the data (red and blue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DE6XFOh4X9A"
   },
   "source": [
    "Before you start anything in machine learning always check your data!!!!!!\n",
    "\n",
    "So we will start be making a simple scatter plot of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpOqS5qC4X9B"
   },
   "outputs": [],
   "source": [
    "colormap = np.array(['r', 'b'])\n",
    "plt.scatter(X[:,0],X[:,1],c=colormap[y])\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ls_xqR-x4X9D"
   },
   "source": [
    "The above plot is the same one as for the example in the book (Example 2.6).\n",
    "\n",
    "Let begin constructing the algorithm...\n",
    "\n",
    "We will start with putting the X and y in the same matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtOBrlix4X9D"
   },
   "outputs": [],
   "source": [
    "data = np.column_stack((X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHNG902efQTN"
   },
   "source": [
    "In the book they do some \"preprocessing\" so they wont need to go through all possible splits. Let us do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHgvYeX04X9F"
   },
   "outputs": [],
   "source": [
    "x1_unq = np.unique(data[:,0]) # We find unique (and sorted) values of the data in column 0 that is \"feature\" x1.\n",
    "split_x1 = (x1_unq[1:]+x1_unq[:-1]) / 2 # From the unique values find where splits for x0 should be using special syntax....\n",
    "x2_unq = np.unique(data[:,1]) # We do the same for column 1 / feature \"x2\"\n",
    "split_x2 = (x2_unq[1:]+x2_unq[:-1]) / 2 #From the unique values find where splits for x0 should be using special syntax...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko8thU8M4X9H"
   },
   "source": [
    "Next thing to do is to decide on a splitting criterion. The book used entropy so lets do the same. We do a quick googling of entropy and find that the good people who have written the SciPy library has implemented this function. So lets use that. We start by importing entropy from scipy. https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cSvPuUQ4X9J"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21FflR2o4X9J"
   },
   "source": [
    "Now for one \"iteration\" of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRB0rQ954X9K"
   },
   "outputs": [],
   "source": [
    "Qtot = np.zeros([len(split_x1)+len(split_x2),3]) # First we \"allocate\" some space\n",
    "k = 0\n",
    "for i in split_x1:\n",
    "    tmp = data[np.where(data[:,0] < i),2]\n",
    "    blues = np.sum(tmp==0)\n",
    "    reds = np.sum(tmp==1)\n",
    "    n1 = blues + reds\n",
    "    pi_1_blue = blues/n1\n",
    "    pi_1_red = reds /n1\n",
    "    Q_1 = entropy([pi_1_blue, pi_1_red])\n",
    "    tmp = data[np.where(data[:,0] >= i),2]\n",
    "    blues = np.sum(tmp==0)\n",
    "    reds = np.sum(tmp==1)\n",
    "    n2 = blues + reds\n",
    "    pi_2_blue = blues/n2\n",
    "    pi_2_red = reds /n2\n",
    "    Q_2 = entropy([pi_2_blue, pi_2_red])\n",
    "    Qtot[k,0] = 0 # x1 feature column\n",
    "    Qtot[k,1] = i # where da split\n",
    "    Qtot[k,2] = n1*Q_1+n2*Q_2\n",
    "    k +=1\n",
    "\n",
    "# Find split in x2 (second feature)\n",
    "x2_unq = np.unique(data[:,1])\n",
    "# From the unique values find where splits should be using special syntax....\n",
    "split_x2 = (x2_unq[1:]+x2_unq[:-1]) / 2\n",
    "for i in split_x2:\n",
    "    tmp = data[np.where(data[:,1] < i),2]\n",
    "    blues = np.sum(tmp==0)\n",
    "    reds = np.sum(tmp==1)\n",
    "    n1 = blues + reds\n",
    "    pi_1_blue = blues/n1\n",
    "    pi_1_red = reds /n1\n",
    "    Q_1 = entropy([pi_1_blue, pi_1_red])\n",
    "    tmp = data[np.where(data[:,1] >= i),2]\n",
    "    blues = np.sum(tmp==0)\n",
    "    reds = np.sum(tmp==1)\n",
    "    n2 = blues + reds\n",
    "    pi_2_blue = blues/n2\n",
    "    pi_2_red = reds /n2\n",
    "    Q_2 = entropy([pi_2_blue, pi_2_red])\n",
    "    Qtot[k,0] = 1 # x1 feature column\n",
    "    Qtot[k,1] = i # where da split\n",
    "    Qtot[k,2] = n1*Q_1+n2*Q_2\n",
    "    k +=1\n",
    "\n",
    "# Now we have all the Entropy measures for all the ways to do the first split and we find the lowest value at\n",
    "first_split = np.argmin(Qtot[:,2])\n",
    "print(\"We split based on the feature in column {:f} when less than {:f} with entropy {:f}\".format(Qtot[first_split,0],Qtot[first_split,1],Qtot[first_split,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-vh6TlU4X9L"
   },
   "source": [
    "We will plot the data again with the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1FrDneV4X9M"
   },
   "outputs": [],
   "source": [
    "colormap = np.array(['r', 'b'])\n",
    "plt.scatter(X[:,0],X[:,1],c=colormap[y])\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.plot([0,10],[Qtot[first_split,1],Qtot[first_split,1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VE8-MMHM4X9N"
   },
   "source": [
    "We have our first split and then we continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35uA3qgw4X9N"
   },
   "outputs": [],
   "source": [
    "# We make a copy of the dataset that is left to split\n",
    "data = data[np.where(data[:,int(Qtot[first_split,0])] > Qtot[first_split,1])]\n",
    "# And then we start over...\n",
    "x1_unq = np.unique(data[:,0])\n",
    "# From the unique values find where splits should be using special syntax....\n",
    "split_x1 = (x1_unq[1:]+x1_unq[:-1]) / 2\n",
    "# Find split in x2 (second feature)\n",
    "x2_unq = np.unique(data[:,1])\n",
    "# From the unique values find where splits should be using special syntax....\n",
    "split_x2 = (x2_unq[1:]+x2_unq[:-1]) / 2\n",
    "Qtot = np.zeros([len(split_x1)+len(split_x2),3])\n",
    "k = 0\n",
    "for i in split_x1:\n",
    "    #print(i)\n",
    "    #tmp = np.where(data[:,0] < i)\n",
    "    #unique, counts = np.unique(data[np.where(data[:,0] < i),2],return_counts=True)\n",
    "    tmp = data[np.where(data[:,0] < i),2]\n",
    "    blues = np.sum(tmp==0)\n",
    "    reds = np.sum(tmp==1)\n",
    "    n1 = blues + reds\n",
    "    pi_1_blue = blues/n1\n",
    "    pi_1_red = reds /n1\n",
    "    Q_1 = entropy([pi_1_blue, pi_1_red])\n",
    "    #unique, counts = np.unique(data[np.where(data[:,0] >= i),2],return_counts=True)\n",
    "    tmp = data[np.where(data[:,0] >= i),2]\n",
    "    blues = np.sum(tmp==0)\n",
    "    reds = np.sum(tmp==1)\n",
    "    n2 = blues + reds\n",
    "    pi_2_blue = blues/n2\n",
    "    pi_2_red = reds /n2\n",
    "    Q_2 = entropy([pi_2_blue, pi_2_red])\n",
    "    Qtot[k,0] = 0 # x1 feature column\n",
    "    Qtot[k,1] = i # where da split\n",
    "    Qtot[k,2] = n1*Q_1+n2*Q_2\n",
    "    k +=1\n",
    "\n",
    "# Find split in x2 (second feature)\n",
    "x2_unq = np.unique(data[:,1])\n",
    "# From the unique values find where splits should be using special syntax....\n",
    "split_x2 = (x2_unq[1:]+x2_unq[:-1]) / 2\n",
    "for i in split_x2:\n",
    "    #print(i)\n",
    "    #tmp = np.where(data[:,0] < i)\n",
    "   # unique, counts = np.unique(data[np.where(data[:,1] < i),2],return_counts=True)\n",
    "    tmp = data[np.where(data[:,1] < i),2]\n",
    "    blues = np.sum(tmp==0)\n",
    "    reds = np.sum(tmp==1)\n",
    "    n1 = blues + reds\n",
    "    pi_1_blue = blues/n1\n",
    "    pi_1_red = reds /n1\n",
    "    Q_1 = entropy([pi_1_blue, pi_1_red])\n",
    "  #  unique, counts = np.unique(data[np.where(data[:,1] >= i),2],return_counts=True)\n",
    "    tmp = data[np.where(data[:,1] >= i),2]\n",
    "    blues = np.sum(tmp==0)\n",
    "    reds = np.sum(tmp==1)\n",
    "    n2 = blues + reds\n",
    "    pi_2_blue = blues/n2\n",
    "    pi_2_red = reds /n2\n",
    "    Q_2 = entropy([pi_2_blue, pi_2_red])\n",
    "    Qtot[k,0] = 1 # x1 feature column\n",
    "    Qtot[k,1] = i # where da split\n",
    "    Qtot[k,2] = n1*Q_1+n2*Q_2\n",
    "    k +=1\n",
    "\n",
    "# Now we all the Entropy measures for all the ways to do the first split we find the lowest value at\n",
    "second_split = np.argmin(Qtot[:,2])\n",
    "print(\"We split based on the feature in column {:f} when less than {:f} with entropy {:f}\".format(Qtot[second_split,0],Qtot[second_split,1],Qtot[second_split,2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZqzpAKx4X9O"
   },
   "source": [
    "We found our second split so lets plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3laOTCjY4X9O"
   },
   "outputs": [],
   "source": [
    "colormap = np.array(['r', 'b'])\n",
    "plt.scatter(X[:,0],X[:,1],c=colormap[y])\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.plot([0,10],[3,3],'g--') # We remembered that the previous split was at y = 3\n",
    "plt.plot([Qtot[second_split,1],Qtot[second_split,1]],[3,10],'c--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUoigVNV4X9P"
   },
   "source": [
    "As you might have realised, a lot of this code is repetition so a lot of the code can be reused using functions and recursion.\n",
    "\n",
    "## The end"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
