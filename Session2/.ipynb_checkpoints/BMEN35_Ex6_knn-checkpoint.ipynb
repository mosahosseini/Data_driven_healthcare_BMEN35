{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35_2023/blob/main/Session2/BMEN35_Ex6_knn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRWK56LV6vbY"
   },
   "source": [
    "## knn (k-nearest neighbour)\n",
    "\n",
    "In this notebook we will get to know the k-nearest neigbour (knn) algorithm for classification. We will use a famous old dataset, the Iris dataset (https://en.wikipedia.org/wiki/Iris_flower_data_set), to illustrate the algorithm. We will start by importing some of the libraries we need and load the data.\n",
    "\n",
    "Sidebar on variable naming of your data. X is a matrix (because uppercase letters are used), y is a vector (because lowercase letters are used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_muO3bD9FYa"
   },
   "outputs": [],
   "source": [
    "import numpy as np # Always good to have...\n",
    "from sklearn.datasets import load_iris # We are going to use the Iris dataset\n",
    "from sklearn.model_selection import train_test_split  # Split them data\n",
    "from sklearn.preprocessing import StandardScaler # Here we will also \"scale\" our data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size = 0.5, random_state=0) # Also note annoying change of variable naming convention. You will find all kinds of conventions...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsbTsoXrJCLX"
   },
   "source": [
    "It is generally good practise to \"scale/normalise\" your data before use (this depends on the algorithm though, see p. 24 in the book on input normalisation). There are many possibilites here, but in this example we will make use of the `StandardScaler()` function. This function removes the means and scales to unit variance for each feature. (Check this for yourself, you should know how to do it, hint: np.mean() and np.var()). Also think about why we do it after the dataset is split into test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zO8NsVMhKDrU"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train) # We use the train data to find the scaling parameters\n",
    "X_test = scaler.transform(X_test) # And use these scaling parameters on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7odTXPvoK3V0"
   },
   "source": [
    "The knn algorithm goes through the whole training dataset and find the n examples that are \"closest\" to the test example. The class associated with training example that is \"closest\" to test example is the output of the algorithm. So we can use a for - loop to go through all the examples. Lets first try to classify the X = [1 1 1 1] using k = 1. As a distance metric we will use the Euclidian distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqAi3mMhLvvN"
   },
   "outputs": [],
   "source": [
    "X = np.array([1,1,1,1]) # This is our X to be tested\n",
    "neighbors = 1 # How many neigbors to use in the algoritm (the k in knn)\n",
    "dist_class = np.zeros([X_train.shape[0],2]) # Allocate a variable where we store the distance metric and which class\n",
    "\n",
    "# First we compute the Euclidean distance between our X and all the datapoints in the training dataset\n",
    "for j in range(X_train.shape[0]): # For every sample in X_train\n",
    "  d = (np.sqrt(np.sum(np.square(X - X_train[j,:])))) # Euclidean distance\n",
    "  dist_class[j,:] = [d, y_train[j]] # Store euclidean distance in first column and class in second column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6IDt0QyTlG9"
   },
   "source": [
    "Now we have a matrix with Euclidian distance in the first column and class in the second column. What we want to do now is to sort the matrix according to the distance values (shortest first). Luckily, there is just the thing to do this available in numpy (argsort()). So we sort the distance and class array/matrix using the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCkfrQzaQzHM"
   },
   "outputs": [],
   "source": [
    "dist_class = dist_class[dist_class[:,0].argsort()] # This function has a somewhat complicated syntax..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxSpxMpxYoR2"
   },
   "source": [
    "Now since we use n = 1 the first value in the second column will be our answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VrDDSjuZBDH"
   },
   "outputs": [],
   "source": [
    "y_hat = dist_class[0,1]\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWHoIQ9es7xN"
   },
   "source": [
    "The answer is that the class is 2. What does this actually mean? In the iris object there is an array named target_names. Lets print them and select the appropriate one. (Note we cast the output to int to be able to index the iris.target_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gD5KSQUQtUEM"
   },
   "outputs": [],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTKvTExXta_E"
   },
   "outputs": [],
   "source": [
    "print(iris.target_names[int(y_hat)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1n-l-xktJc_"
   },
   "source": [
    "Change the values of X to [5.6 2.7 4.2 1.3] and check what you get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NImWGOKRaZ0w"
   },
   "source": [
    "This is all good but we might want to do this for the entire X_test matrix (75 rows with 4 columns of data). To do this we can use another for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pn61jerLa28I"
   },
   "outputs": [],
   "source": [
    "from scipy import stats # In order to compute the \"mode\" a.k.a. most occuring value\n",
    "\n",
    "neighbor = 3 # We will also use 3 neighbors\n",
    "dist_class = np.zeros([X_test.shape[0],2]) # Allocate space for distance and class\n",
    "y_hat = np.zeros([X_test.shape[0],]) # Allocate space for prediction\n",
    "for i in range(X_test.shape[0]): # For every sample in X_test\n",
    "  for j in range(X_train.shape[0]): # For every sample in X_train\n",
    "    d = (np.sqrt(np.sum(np.square(X_test[i,:] - X_train[j,:])))) # Euclidean distance\n",
    "    dist_class[j,:] = [d, y_train[j]] # Store euclidean distance in first column and class in second column\n",
    "\n",
    "\n",
    "  dist_class = dist_class[dist_class[:,0].argsort()] # Sort the array in ascending order based on Euclidean distance , closest first, as before\n",
    "  y_hat[i] = stats.mode(dist_class[0:neighbor,1])[0] # Extract the most common value (mode in stats speak) of the array of k nearest neighbors\n",
    "\n",
    "\n",
    "\n",
    "accuracy = np.sum(y_hat==y_test)/y_test.size # Calculate the classification accuracy\n",
    "print(\"The accuracy, using {:d} neighbors is {:f}\".format(neighbor, accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWODKSp_dGvc"
   },
   "source": [
    "What would you change to compute the classification error instead??\n",
    "\n",
    "Now we can also check what number of neighbours gives us the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ex7Zph9TdnL8"
   },
   "outputs": [],
   "source": [
    "from scipy import stats # In order to compute the \"mode\" a.k.a. most occuring value\n",
    "from sklearn.metrics import accuracy_score # How well did we do...\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy = np.zeros(11)\n",
    "for neighbor in range(1,11):\n",
    "  dist_class = np.zeros([X_test.shape[0],2]) # Allocate space for distance and class\n",
    "  y_hat = np.zeros([X_test.shape[0],]) # Allocate space for prediction\n",
    "  for i in range(X_test.shape[0]): # For every sample in X_test\n",
    "    for j in range(X_train.shape[0]): # For every sample in X_train\n",
    "      d = (np.sqrt(np.sum(np.square(X_test[i,:] - X_train[j,:])))) # Euclidean distance\n",
    "      dist_class[j,:] = [d, y_train[j]] # Store euclidean distance in first column and class in second column\n",
    "\n",
    "\n",
    "    dist_class = dist_class[dist_class[:,0].argsort()] # Sort the array in ascending order based on Euclidean distance , closest first, as before\n",
    "    y_hat[i] = stats.mode(dist_class[0:neighbor,1])[0] # Extract the most common value (mode in stats speak) of the array of k nearest neighbors\n",
    "\n",
    "  accuracy[neighbor] = accuracy_score(y_test, y_hat) # There is of course a method already available to compute this\n",
    "\n",
    "\n",
    "plt.plot(accuracy[1:10])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HSx2O22vu8m"
   },
   "source": [
    "That is all good! Next thing to do is to try knn regression. To do this we only need to modify one single little thing in the code from before. Think about what you would change!\n",
    "\n",
    "We will make up our own data for this using a function from sklearn (more about sklearn in the next notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtuuvXj5w1os"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X,y = make_regression(1000,4, random_state = 1) # We make 1000 examples with 4 features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 1) # Split the dataset into training and testing\n",
    "# We scale the data like in previous example\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "from sklearn.metrics import mean_squared_error, r2_score # Regression metrics, yay!!\n",
    "\n",
    "# We copy the code from a previous cell\n",
    "neighbor = 3 # We will also use 3 neighbors\n",
    "dist_class = np.zeros([X_test.shape[0],2]) # Allocate space for distance and class\n",
    "y_hat = np.zeros([X_test.shape[0],]) # Allocate space for prediction\n",
    "for i in range(X_test.shape[0]): # For every sample in X_test\n",
    "  for j in range(X_train.shape[0]): # For every sample in X_train\n",
    "    d = (np.sqrt(np.sum(np.square(X_test[i,:] - X_train[j,:])))) # Euclidean distance\n",
    "    dist_class[j,:] = [d, y_train[j]] # Store euclidean distance in first column and class in second column\n",
    "\n",
    "\n",
    "  dist_class = dist_class[dist_class[:,0].argsort()] # Sort the array in ascending order based on Euclidean distance , closest first, as before\n",
    "  y_hat[i] = np.mean(dist_class[0:neighbor,1]) # This is where the change is! We use the mean of the k neighbors\n",
    "\n",
    "mse = np.mean((y_hat-y_test)**2) # Mean squared error\n",
    "mae = np.mean(np.abs(y_hat-y_test)) # Mean absolute error\n",
    "r2 =  1 - (np.sum((y_test-y_hat)**2))/(np.sum((y_test - np.mean(y_test))**2))# R2 or coefficient or determination\n",
    "print(\"The Mean Squared Error is {:f} and the R2 (Coefficient of determination) is {:f}, using {:d} neighbors\".format(mse, r2, neighbor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0og2dd68AN1W"
   },
   "source": [
    "You have finished this tutorial\n",
    "\n",
    "## The end"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
