{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35_2023/blob/main/Session2/BMEN35_Ex7_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESjhQIqlNZ29"
   },
   "source": [
    "## scikit-learn\n",
    "\n",
    "In this tutorial we will be looking at one of the machine learning libraries in Python, namely scikit-learn (a.k.a. sklearn). This notebook covers some of the basics of sklearn and it will be used a lot in this course.\n",
    "\n",
    "Scikit-learn (https://scikit-learn.org/) is a free machine learning library for Python. It has algorithms for classification, regression and clustering as well as a lot of algorithms for preparing data for machine learning algorithms.\n",
    "\n",
    "If you are running this code on your own (outside google colab), you will need to install the scikit-learn package (eg. using pip or conda). We will start with the \"standard\" way of importing scikit-learn (given that it is installed).\n",
    "\n",
    "We will use the Iris dataset (our favourite) https://https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset as an example. This is one of the best known dataset in the field of machine learning. See also https://en.wikipedia.org/wiki/Iris_flower_data_set . In short there are 3 different classes of flower (Setosa, Veriscolor, Virginica), there are 4 properties measured on each flower (sepal length and width and petal length and width). There are 50 examples of each class of flower (150 examples in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7_RJojtjNQrg"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris # There are several datasets available in sklearn\n",
    "iris = load_iris() # The data is now loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOwDy-qr0ue2"
   },
   "source": [
    "Sklearn has a very useful function to \"split\" your dataset into a test and train part. This can be performed using the `train_test_split()` function from sklearn.model_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7ZELrOgZ1BvS"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTCBTyu31daW"
   },
   "source": [
    "If you now check the Variables \"tab\" {x} (if in colab) on the left you will see you have split the dataset into a training part and testing part each consisting of 50% of the original dataset. Now we might want to perform some classfication on this data. sklearn has a \"baseline\" classfier named `DummyClassifier()`. This can be seen as the most basic on classifiers and you should be able to construct something the is better than this. As usual we will start be importing the DummyClassifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iangOjp71vCc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testscore using dummy classifier is 0.280000 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score # More on this one later...\n",
    "dummy_clf = DummyClassifier() # Here we define the classifier, the constructor\n",
    "dummy_clf.fit(X_train,y_train) # Here we \"fit\" the classifier to the data\n",
    "y_hat = dummy_clf.predict(X_test) # Here we predict values using our dummy classifier on the test data\n",
    "test_score = accuracy_score(y_test, y_hat)\n",
    "print('Testscore using dummy classifier is {:f} '.format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WA3mC3tu5pBA"
   },
   "source": [
    "That is not a great score. Close to 1/3. On the other it is a dummy classifier.\n",
    "\n",
    "#The important thing to note here is that in sklearn you basically need three lines of code to perform classification (or regression).\n",
    "1. constructor for the estimator (eg. clf = DummyClassifier()\n",
    "2. fit method for the estimator (eg. clf.fit(X_train, y_train) # This is the \"training\" part\n",
    "3. predict method for the estimator (eg. y_hat = clf.predict(X_test) # This is the prediction part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp978c9d5HJ0"
   },
   "source": [
    "You might also have noticed the use of `sklearn.metrics`. You can find more information about this here https://scikit-learn.org/stable/modules/model_evaluation.html . Basically, there are built-in functions for computing accuracy, f1 score, precision, recall etc.\n",
    "\n",
    "## The end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuiiQBpy5HJ1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
