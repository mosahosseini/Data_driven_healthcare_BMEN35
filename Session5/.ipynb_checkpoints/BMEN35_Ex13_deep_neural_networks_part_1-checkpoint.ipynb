{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/antfolk/BMEN35_2023/blob/main/Session5/BMEN35_Ex13_deep_neural_networks_part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC8M5NjP948E"
   },
   "source": [
    "# Deep Neural Networks part 1\n",
    "\n",
    "In this notebook we will load the data, scale the data, make a model using a Convolutional Neural Network and train it. This notebook contains quite a lot of code. Read it, run it and try to get an understanding of how it works. In these notebooks we will use the MNIST dataset (https://en.wikipedia.org/wiki/MNIST_database). It is a database of handwritten digits, has (originally) a training set of 60000 examples and a test set of 10000 examples. The images are greyscale images of 28x28 pixels.\n",
    "\n",
    "Let's start with importing our usual suspects (modules/libraries) and a few new ones.\n",
    "\n",
    "### Have some patience with this notebook as training may take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sOzD6kXlEnWw"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6744\\2857115590.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# The usual imports and a few others\n",
    "import numpy as np\n",
    "#from google.colab import files\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5XQRJNrEu2c"
   },
   "source": [
    "Now when all of that is done, we will load the dataset and check it out, format and scale the data appropriately. The dataset is available in Keras using `mnist.load_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tjZDfM7KF30Y"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6744\\4088039192.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X_train shape : {:}, y_train shape {:}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X_test shape : {:}, y_test shape {:}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"X_train shape : {:}, y_train shape {:}\".format(X_train.shape, y_train.shape))\n",
    "print(\"X_test shape : {:}, y_test shape {:}\".format(X_test.shape, y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CakWDN22G89L"
   },
   "source": [
    "So, there are alot of examples here. like we stated earlier (60k for training and 10k for test). We will cheat a bit and not use the whole dataset (this is to save some time in training).\n",
    "\n",
    " We will also have a look at some of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUkm5TVUG8UT"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 9))\n",
    "for i in range(9):\n",
    "  # define subplot\n",
    "  plt.subplot(3,3,i+1)\n",
    "  # plot raw pixel data\n",
    "  plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "  plt.title(\"Class {}\".format(y_train[i]))\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4dkGd6wIOPM"
   },
   "source": [
    "Here we will select a subset of the data (10% each of test and train), reshape the data (to fit with Keras models we use later) and use one-hot encoding for the targets/classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "427SgFEYHY4e"
   },
   "outputs": [],
   "source": [
    "# We will only use a subsection of the data to save some time in training,\n",
    "# normally you would use the whole dataset\n",
    "a = np.random.choice(60000, 6000, replace=False)\n",
    "b = np.random.choice(10000, 1000, replace=False)\n",
    "X_train = X_train[a,:,:]\n",
    "y_train = y_train[a]\n",
    "X_test = X_test[b,:,:]\n",
    "y_test = y_test[b]\n",
    "# Here we need to reshape the data to work with input layers\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "# We will use one hot encoding for the target/classes values\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5Etk7HTJHXo"
   },
   "source": [
    "Next we will do some more formatting of the data. We will convert the data to floats (floating point representation) and scale the pixel values to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea7hPWWJJZch"
   },
   "outputs": [],
   "source": [
    "# Cast to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize to range 0-1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKBXfE0SHYj5"
   },
   "source": [
    "Now that all that is done, let's start with defining our model. We will use the Sequential API (`Sequential()` ) from Keras (https://keras.io/guides/sequential_model/). We will add some layers and define our optimiser, loss function and metrics to be saved. We will also make a plot to visualize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CCE8vdaJ-Hr"
   },
   "outputs": [],
   "source": [
    "# We will use the Sequential model\n",
    "model = Sequential()\n",
    "# We will add some layers to this model\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# compile model\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "plot_model(model, show_shapes=True, show_layer_names=True, show_layer_activations=True) # This also generates a file of the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZlFG_5ZHYf5"
   },
   "source": [
    "Now we have defined our model, let's start with training. The fit method returns a history object where values for training loss and other metrics can be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RklPX6tCNkfI"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCUd2X46O-oV"
   },
   "source": [
    "Now the model is trained, we will check the results for both the training and test set. We will also save the model to the current workspace and download it to your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_m2aw34iN_Is"
   },
   "outputs": [],
   "source": [
    "train_results = model.evaluate(X_train, y_train, verbose=0)\n",
    "print(\"Training accuracy is {:.4f} \".format(train_results[1]))\n",
    "test_results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy is {:.4f} \".format(test_results[1]))\n",
    "# Lets also save our model\n",
    "model.save('first_try.h5') # This saves the model as a file in the current workspace\n",
    "files.download( \"first_try.h5\" ) # This will download the model to your computer, if you are running this locally on your computer skip this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdfN4SVROwkm"
   },
   "source": [
    "We can also plot some of the results, the loss and the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSgqPau-O0mn"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "# plot accuracy\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'], color='blue', label='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdonfhUYQXZq"
   },
   "source": [
    "What we can add now, is to use cross-validation to estimate the accuracy. We will still use the same model. The training here will take some time so have patience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc-JRN40Qu4s"
   },
   "outputs": [],
   "source": [
    "n_folds = 5 # Number of folds\n",
    "dataX = X_train\n",
    "dataY = y_train\n",
    "scores, histories = list(), list()\n",
    "# prepare cross validation\n",
    "kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "best_model = []\n",
    "best_score = 0\n",
    "for train_ix, test_ix in kfold.split(dataX):\n",
    "  # define model\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "  model.add(MaxPooling2D((2, 2)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "  model.add(Dense(10, activation='softmax'))\n",
    "  # compile model\n",
    "  opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  # select rows for train and test\n",
    "  trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "  # fit model\n",
    "  history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "  # evaluate model\n",
    "  test_results = model.evaluate(testX, testY, verbose=0)\n",
    "  print(\"Test accuracy is {:.4f} \".format(test_results[1]))\n",
    "  # stores scores\n",
    "  scores.append(test_results[1])\n",
    "  histories.append(history)\n",
    "  if test_results[1] > best_score:\n",
    "    best_score = test_results[1]\n",
    "    best_model = model\n",
    "\n",
    "best_model.save('model_a.h5')\n",
    "files.download( \"model_a.h5\" ) # This will download the model to your computer, if you are running this locally on your computer skip this line\n",
    "plot_model(best_model, show_shapes=True, show_layer_names=True, show_layer_activations=True) # This also generates a file of the plot\n",
    "#best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUUyKcv4bM31"
   },
   "source": [
    "Now we have trained using five-fold cross validation. Lets plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNslBPX3bUZt"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 7))\n",
    "for i in range(len(histories)):\n",
    "  # plot loss\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.title('Cross Entropy Loss')\n",
    "  plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "  # plot accuracy\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.title('Classification Accuracy')\n",
    "  plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtU8-ygecM0w"
   },
   "source": [
    "Finally, we can summarize the performance using a boxplot and by computing the mean and standard deviation of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHk45Je9cVVY"
   },
   "outputs": [],
   "source": [
    "# print summary\n",
    "print('Accuracy: mean= {:.4f} std= {:.4f}, n= {:d}'.format(mean(scores)*100, std(scores)*100, len(scores)))\n",
    "# box and whisker plots of results\n",
    "plt.boxplot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTL7k8DtdW_s"
   },
   "source": [
    "We will now train another model (which we will name model_b) and add batch normalisation layers. We will use the same procedure as before and use crossvalidation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kAOJhNZNZ7j"
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "dataX = X_train\n",
    "dataY = y_train\n",
    "scores, histories = list(), list()\n",
    "# prepare cross validation\n",
    "kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "best_model = []\n",
    "best_score = 0\n",
    "for train_ix, test_ix in kfold.split(dataX):\n",
    "  # define model\n",
    "  model_b = Sequential()\n",
    "  model_b.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "  model_b.add(BatchNormalization())\n",
    "  model_b.add(MaxPooling2D((2, 2)))\n",
    "  model_b.add(Flatten())\n",
    "  model_b.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "  model_b.add(BatchNormalization())\n",
    "  model_b.add(Dense(10, activation='softmax'))\n",
    "  # compile model\n",
    "  opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  # compile model\n",
    "  opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "  model_b.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  # select rows for train and test\n",
    "  trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "  # fit model\n",
    "  history = model_b.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "  # evaluate model\n",
    "  test_results = model_b.evaluate(testX, testY, verbose=0)\n",
    "  print(\"Test accuracy is {:.4f} \".format(test_results[1]))\n",
    "    # stores scores\n",
    "  scores.append(test_results[1])\n",
    "  histories.append(history)\n",
    "  if test_results[1] > best_score:\n",
    "    best_score = test_results[1]\n",
    "    best_model = model_b\n",
    "\n",
    "\n",
    "best_model.save('model_b.h5')\n",
    "files.download( \"model_b.h5\" ) # This will download the model to your computer, if you are running this locally on your computer skip this line\n",
    "plot_model(best_model, show_shapes=True, show_layer_names=True, show_layer_activations=True) # This also generates a file of the plot\n",
    "#best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Il89dDG_hLgR"
   },
   "source": [
    "Now we have trained the models (model_b), selected the best one, saved it and now we can plot the results in a similar way as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDhjHEvEhhPj"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 7))\n",
    "for i in range(len(histories)):\n",
    "  # plot loss\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.title('Cross Entropy Loss')\n",
    "  plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "  # plot accuracy\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.title('Classification Accuracy')\n",
    "  plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usu2WTG_i2nA"
   },
   "outputs": [],
   "source": [
    "# print summary\n",
    "print('Accuracy: mean= {:.4f} std= {:.4f}, n= {:d}'.format(mean(scores)*100, std(scores)*100, len(scores)))\n",
    "# box and whisker plots of results\n",
    "plt.boxplot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hplVi9iajAJ7"
   },
   "source": [
    "We will now train yet another model with another configuration of layers, this will be our \"model c\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNiZdf2bhmDf"
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "dataX = X_train\n",
    "dataY = y_train\n",
    "scores, histories = list(), list()\n",
    "# prepare cross validation\n",
    "kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "best_model = []\n",
    "best_score = 0\n",
    "for train_ix, test_ix in kfold.split(dataX):\n",
    "  # define model\n",
    "  model_c = Sequential()\n",
    "  model_c.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "  model_c.add(MaxPooling2D((2, 2)))\n",
    "  model_c.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "  model_c.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "  model_c.add(MaxPooling2D((2, 2)))\n",
    "  model_c.add(Flatten())\n",
    "  model_c.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "  model_c.add(Dense(10, activation='softmax'))\n",
    "  # compile model\n",
    "  opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "  model_c.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  # select rows for train and test\n",
    "  trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "  # fit model\n",
    "  history = model_c.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "  # evaluate model\n",
    "  test_results = model_c.evaluate(testX, testY, verbose=0)\n",
    "  print(\"Test accuracy is {:.4f} \".format(test_results[1]))\n",
    "    # stores scores\n",
    "  scores.append(test_results[1])\n",
    "  histories.append(history)\n",
    "  if test_results[1] > best_score:\n",
    "    best_score = test_results[1]\n",
    "    best_model = model_c\n",
    "\n",
    "best_model.save('model_c.h5')\n",
    "files.download( \"model_c.h5\" ) # This will download the model to your computer, if you are running this locally on your computer skip this line\n",
    "plot_model(best_model, show_shapes=True, show_layer_names=True, show_layer_activations=True) # This also generates a file of the plot\n",
    "#best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWZqXjonlxeT"
   },
   "source": [
    "Now we have trained the models (model_c), selected the best one, saved it and now we can plot the results in a similar way as we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dp_ATsX6hl6i"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 7))\n",
    "for i in range(len(histories)):\n",
    "  # plot loss\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.title('Cross Entropy Loss')\n",
    "  plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "  # plot accuracy\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.title('Classification Accuracy')\n",
    "  plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEo3s64zhlzZ"
   },
   "outputs": [],
   "source": [
    "# print summary\n",
    "print('Accuracy: mean= {:.4f} std= {:.4f}, n= {:d}'.format(mean(scores)*100, std(scores)*100, len(scores)))\n",
    "# box and whisker plots of results\n",
    "plt.boxplot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCF3s72Xl-j7"
   },
   "source": [
    "We will now train yet another model with another configuration of layers, here we will also use strides. This will be our \"model d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXRJfq9dhlq1"
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "dataX = X_train\n",
    "dataY = y_train\n",
    "scores, histories = list(), list()\n",
    "# prepare cross validation\n",
    "kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "best_model = []\n",
    "best_score = 0\n",
    "for train_ix, test_ix in kfold.split(dataX):\n",
    "  # define model\n",
    "  model_d = Sequential()\n",
    "  model_d.add(Conv2D(4, (5, 5), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "  model_d.add(Conv2D(8, (5, 5), activation='relu', kernel_initializer='he_uniform', strides=(2, 2)))\n",
    "  model_d.add(Conv2D(12, (4, 4), activation='relu', kernel_initializer='he_uniform', strides=(2, 2)))\n",
    "  model_d.add(Flatten())\n",
    "  model_d.add(Dense(200, activation='relu', kernel_initializer='he_uniform'))\n",
    "  model_d.add(Dense(10, activation='softmax'))\n",
    "  # compile model\n",
    "  opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "  model_d.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  # select rows for train and test\n",
    "  trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "  # fit model\n",
    "  history = model_d.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "  # evaluate model\n",
    "  test_results = model_d.evaluate(testX, testY, verbose=0)\n",
    "  print(\"Test accuracy is {:.4f} \".format(test_results[1]))\n",
    "    # stores scores\n",
    "  scores.append(test_results[1])\n",
    "  histories.append(history)\n",
    "  if test_results[1] > best_score:\n",
    "    best_score = test_results[1]\n",
    "    best_model = model_d\n",
    "\n",
    "best_model.save('model_d.h5')\n",
    "files.download( \"model_d.h5\" ) # This will download the model to your computer, if you are running this locally on your computer skip this line\n",
    "plot_model(best_model, show_shapes=True, show_layer_names=True, show_layer_activations=True) # This also generates a file of the plot\n",
    "#best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIgypO8tnvl4"
   },
   "source": [
    "Now we have trained the models (model_d), selected the best one, saved it and now we can plot the results in a similar way as we did earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-pF5k3Nhlay"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 7))\n",
    "for i in range(len(histories)):\n",
    "  # plot loss\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.title('Cross Entropy Loss')\n",
    "  plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "  # plot accuracy\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.title('Classification Accuracy')\n",
    "  plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dCwHGE53n7CK"
   },
   "outputs": [],
   "source": [
    "# print summary\n",
    "print('Accuracy: mean= {:.4f} std= {:.4f}, n= {:d}'.format(mean(scores)*100, std(scores)*100, len(scores)))\n",
    "# box and whisker plots of results\n",
    "plt.boxplot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCm2xYKPn91Z"
   },
   "source": [
    "Next we will implement a one-layer network (similar to Logistic Regression).For this part we also need to reshape our data. This will be our \"model e\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fAMPOizo4yJ"
   },
   "outputs": [],
   "source": [
    "# We will also have to \"reshape\" our data\n",
    "n_folds = 5\n",
    "num_pixels = 784\n",
    "dataX = X_train.reshape((X_train.shape[0], num_pixels))\n",
    "dataY = y_train\n",
    "scores, histories = list(), list()\n",
    "# prepare cross validation\n",
    "kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "best_model = []\n",
    "best_score = 0\n",
    "for train_ix, test_ix in kfold.split(dataX):\n",
    "  # define model\n",
    "  model_e = Sequential()\n",
    "  model_e.add(Dense(10, input_dim=784, activation='softmax'))\n",
    "  # compile model\n",
    "  opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "  model_e.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  # compile model\n",
    "  opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "  model_e.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  # select rows for train and test\n",
    "  trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "  # fit model\n",
    "  history = model_e.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "  # evaluate model\n",
    "  test_results = model_e.evaluate(testX, testY, verbose=0)\n",
    "  print(\"Test accuracy is {:.4f} \".format(test_results[1]))\n",
    "    # stores scores\n",
    "  scores.append(test_results[1])\n",
    "  histories.append(history)\n",
    "  if test_results[1] > best_score:\n",
    "    best_score = test_results[1]\n",
    "    best_model = model_e\n",
    "\n",
    "best_model.save('model_e.h5')\n",
    "files.download( \"model_e.h5\" ) # This will download the model to your computer, if you are running this locally on your computer skip this line\n",
    "plot_model(best_model, show_shapes=True, show_layer_names=True, show_layer_activations=True) # This also generates a file of the plot\n",
    "#best_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91m-K35OqGnT"
   },
   "source": [
    "Now we have trained the models (model_e), selected the best one, saved it and now we can plot the results in a similar way as we did earlier. That was much quicker!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJCOx_SiqJEJ"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 7))\n",
    "for i in range(len(histories)):\n",
    "  # plot loss\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.title('Cross Entropy Loss')\n",
    "  plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "  # plot accuracy\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.title('Classification Accuracy')\n",
    "  plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCn7OQunn9Mu"
   },
   "outputs": [],
   "source": [
    "# print summary\n",
    "print('Accuracy: mean= {:.4f} std= {:.4f}, n= {:d}'.format(mean(scores)*100, std(scores)*100, len(scores)))\n",
    "# box and whisker plots of results\n",
    "plt.boxplot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on_R2UmsqOjO"
   },
   "source": [
    "Now we will make simple neural network, similar to what you could do with MLPClassifier in sklearn. This will be our \"model f\". Also here we need to reshape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVbCzcpbqWSw"
   },
   "outputs": [],
   "source": [
    "# We will first have to \"reshape\" our data\n",
    "n_folds = 5\n",
    "num_pixels = 784\n",
    "dataX = X_train.reshape((X_train.shape[0], num_pixels))\n",
    "dataY = y_train\n",
    "scores, histories = list(), list()\n",
    "# prepare cross validation\n",
    "kfold = KFold(n_folds, shuffle=True, random_state=1)\n",
    "# enumerate splits\n",
    "best_model = []\n",
    "best_score = 0\n",
    "for train_ix, test_ix in kfold.split(dataX):\n",
    "  # define model\n",
    "  model_f = Sequential()\n",
    "  model_f.add(Dense(200, input_dim=784, activation='relu'))\n",
    "  model_f.add(Dense(10, activation='softmax'))\n",
    "  # compile model\n",
    "  opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "  model_f.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  # select rows for train and test\n",
    "  trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]\n",
    "  # fit model\n",
    "  history = model_f.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)\n",
    "  # evaluate model\n",
    "  test_results = model_f.evaluate(testX, testY, verbose=0)\n",
    "  print(\"Test accuracy is {:.4f} \".format(test_results[1]))\n",
    "    # stores scores\n",
    "  scores.append(test_results[1])\n",
    "  histories.append(history)\n",
    "  if test_results[1] > best_score:\n",
    "    best_score = test_results[1]\n",
    "    best_model = model_f\n",
    "\n",
    "best_model.save('model_f.h5')\n",
    "files.download( \"model_f.h5\" ) # This will download the model to your computer, if you are running this locally on your computer skip this line\n",
    "plot_model(best_model, show_shapes=True, show_layer_names=True, show_layer_activations=True) # This also generates a file of the plot\n",
    "#best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gr33n-FequFf"
   },
   "source": [
    "Now we have trained the models (model_f), selected the best one, saved it and now we can plot the results in a similar way as we did earlier. That was also relatively quick!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ts2DgvDwyGuP"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 7))\n",
    "for i in range(len(histories)):\n",
    "  # plot loss\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.title('Cross Entropy Loss')\n",
    "  plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_loss'], color='orange', label='test')\n",
    "  # plot accuracy\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.title('Classification Accuracy')\n",
    "  plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
    "  plt.plot(histories[i].history['val_accuracy'], color='orange', label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StY8nsl1yJOs"
   },
   "outputs": [],
   "source": [
    "# print summary\n",
    "print('Accuracy: mean= {:.4f} std= {:.4f}, n= {:d}'.format(mean(scores)*100, std(scores)*100, len(scores)))\n",
    "# box and whisker plots of results\n",
    "plt.boxplot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2Pdv_ZTrF3G"
   },
   "source": [
    "That is it for this notebook. Make sure you have downloaded models a - f to your computer as they will be used in the next notebook.\n",
    "\n",
    "## The end"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
